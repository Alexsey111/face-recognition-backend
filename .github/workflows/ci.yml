# =============================================================================
# CI/CD Pipeline for Face Recognition Service (Optimized for Ubuntu 24.04)
# =============================================================================

name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      skip_tests:
        description: 'Skip tests (use with caution)'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.12'
  DOCKER_REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  MIN_COVERAGE: 80
  TORCH_INDEX_URL: https://download.pytorch.org/whl/cpu

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  packages: write
  security-events: write
  id-token: write
  pull-requests: write

# =============================================================================
# Jobs
# =============================================================================

jobs:
  # --------------------------------------------------------------------------
  # Code Quality & Linting
  # --------------------------------------------------------------------------
  lint:
    name: Code Quality Check
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-lint-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-lint-
            ${{ runner.os }}-pip-

      - name: Install linting dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install flake8 black isort mypy pylint bandit

      - name: Run Black (code formatting)
        run: |
          black --check app/ tests/ || {
            echo "::error::Code formatting check failed. Run 'black app/ tests/' to fix."
            exit 1
          }

      - name: Run isort (import sorting)
        run: |
          isort --check-only app/ tests/ || {
            echo "::error::Import sorting check failed. Run 'isort app/ tests/' to fix."
            exit 1
          }

      - name: Run Flake8 (linting)
        run: |
          flake8 app/ tests/ \
            --max-line-length=120 \
            --extend-ignore=E203,W503,E501 \
            --exclude=__pycache__,.git,__init__.py \
            --count \
            --statistics

      - name: Run MyPy (type checking)
        run: mypy app/ --ignore-missing-imports --no-strict-optional
        continue-on-error: true

      - name: Run Pylint
        run: |
          pylint app/ \
            --disable=C0111,R0903,C0103,R0913,R0914 \
            --max-line-length=120 \
            --output-format=text
        continue-on-error: true

      - name: Run Bandit (security linting)
        run: |
          bandit -r app/ \
            -f json \
            -o bandit-report.json \
            -ll \
            --exclude app/tests
        continue-on-error: true

      - name: Upload Bandit report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bandit-report
          path: bandit-report.json
          retention-days: 30

  # --------------------------------------------------------------------------
  # Security Scanning
  # --------------------------------------------------------------------------
  security:
    name: Security Scan
    runs-on: ubuntu-24.04
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-security-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-security-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install -r requirements.txt
          pip install safety pip-audit

      - name: Run Safety (check dependencies)
        run: |
          safety check \
            --json \
            --output safety-report.json \
            --continue-on-error || true

      - name: Run pip-audit
        run: |
          pip-audit \
            --requirement requirements.txt \
            --format json \
            --output pip-audit-report.json || true

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            safety-report.json
            pip-audit-report.json
          retention-days: 30

      - name: Run Trivy (filesystem scanning)
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # --------------------------------------------------------------------------
  # Unit Tests
  # --------------------------------------------------------------------------
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-24.04
    timeout-minutes: 20
    if: ${{ !inputs.skip_tests }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-unit-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-unit-
            ${{ runner.os }}-pip-

      - name: Install system dependencies (Ubuntu 24.04)
        run: |
          sudo apt-get update
          sudo apt-get install -y libgl1 libglib2.0-0

      - name: Install dependencies (CPU PyTorch)
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install torch torchvision --index-url ${{ env.TORCH_INDEX_URL }}
          pip install -r requirements.txt
          if [ -f requirements-dev.txt ]; then
            pip install -r requirements-dev.txt
          else
            pip install pytest pytest-cov pytest-asyncio pytest-mock
          fi

      - name: Create necessary directories
        run: |
          mkdir -p models logs test_data

      - name: Run pytest (unit tests)
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          pytest tests/unit/ \
            -v \
            --tb=short \
            --cov=app \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --cov-fail-under=${{ env.MIN_COVERAGE }} \
            --junit-xml=junit-report.xml \
            --maxfail=5

      - name: Check coverage threshold
        run: |
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); root = tree.getroot(); print(root.attrib['line-rate'])")
          COVERAGE_PCT=$(python -c "print(int(float($COVERAGE) * 100))")
          echo "Coverage: ${COVERAGE_PCT}%"
          if [ ${COVERAGE_PCT} -lt ${{ env.MIN_COVERAGE }} ]; then
            echo "::error::Coverage ${COVERAGE_PCT}% is below minimum ${{ env.MIN_COVERAGE }}%"
            exit 1
          fi

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-unit
          fail_ci_if_error: false

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: |
            junit-report.xml
            htmlcov/
            coverage.xml
          retention-days: 30

      - name: Comment PR with coverage
        if: github.event_name == 'pull_request'
        uses: py-cov-action/python-coverage-comment-action@v3
        with:
          GITHUB_TOKEN: ${{ github.token }}

  # --------------------------------------------------------------------------
  # Integration Tests
  # --------------------------------------------------------------------------
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-24.04
    timeout-minutes: 30
    needs: unit-tests
    if: ${{ !inputs.skip_tests }}
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-integration-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-integration-
            ${{ runner.os }}-pip-

      - name: Install system dependencies (Ubuntu 24.04)
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libheif-dev \
            libde265-dev \
            libgl1 \
            libglib2.0-0 \
            postgresql-client \
            redis-tools

      - name: Install Python dependencies (CPU PyTorch)
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install torch torchvision --index-url ${{ env.TORCH_INDEX_URL }}
          pip install -r requirements.txt
          if [ -f requirements-dev.txt ]; then
            pip install -r requirements-dev.txt
          else
            pip install pytest pytest-cov pytest-asyncio pytest-mock
          fi

      - name: Wait for services
        run: |
          timeout 30 bash -c 'until pg_isready -h localhost -p 5432; do sleep 1; done'
          timeout 30 bash -c 'until redis-cli -h localhost -p 6379 ping; do sleep 1; done'

      - name: Setup database
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db
        run: |
          if [ -f alembic.ini ]; then
            alembic upgrade head
          fi

      - name: Download models
        run: |
          mkdir -p models
          if [ -f scripts/download_model.py ]; then
            python scripts/download_model.py --model facenet --model minifasnetv2 || echo "Model download failed, continuing..."
          fi

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
          PYTHONPATH: ${{ github.workspace }}
        run: |
          pytest tests/integration/ \
            -v \
            --tb=short \
            --cov=app \
            --cov-report=xml \
            --cov-report=html \
            --junit-xml=junit-integration.xml \
            --maxfail=3

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: integration
          name: codecov-integration
          fail_ci_if_error: false

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            junit-integration.xml
            htmlcov/
          retention-days: 30

  # --------------------------------------------------------------------------
  # Performance Tests
  # --------------------------------------------------------------------------
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-24.04
    timeout-minutes: 30
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-performance-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-performance-
            ${{ runner.os }}-pip-

      - name: Install dependencies (CPU PyTorch)
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install torch torchvision --index-url ${{ env.TORCH_INDEX_URL }}
          pip install -r requirements.txt
          if [ -f requirements-dev.txt ]; then
            pip install -r requirements-dev.txt
          else
            pip install pytest pytest-benchmark
          fi

      - name: Run performance tests
        run: |
          if [ -f tests/performance/test_performance_requirements.py ]; then
            pytest tests/performance/test_performance_requirements.py \
              -v \
              --junit-xml=junit-performance.xml
          else
            echo "::warning::Performance tests not found, skipping"
          fi

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: junit-performance.xml
          retention-days: 30

  # --------------------------------------------------------------------------
  # Build Docker Image
  # --------------------------------------------------------------------------
  build:
    name: Build Docker Image
    runs-on: ubuntu-24.04
    timeout-minutes: 45
    needs: [lint, security]
    permissions:
      contents: read
      packages: write
    outputs:
      image_tags: ${{ steps.meta.outputs.tags }}
      image_digest: ${{ steps.build.outputs.digest }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILD_DATE=${{ github.event.head_commit.timestamp }}
            VCS_REF=${{ github.sha }}
            VERSION=${{ github.ref_name }}
          platforms: linux/amd64

      - name: Generate SBOM
        uses: anchore/sbom-action@v0
        with:
          image: ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}
          format: spdx-json
          output-file: sbom.spdx.json

      - name: Upload SBOM
        uses: actions/upload-artifact@v4
        with:
          name: sbom
          path: sbom.spdx.json
          retention-days: 90

      - name: Scan Docker image with Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}
          format: 'sarif'
          output: 'trivy-image-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload Trivy results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-image-results.sarif'

  # --------------------------------------------------------------------------
  # Validate Metrics (FAR/FRR)
  # --------------------------------------------------------------------------
  validate-metrics:
    name: Validate FAR/FRR Metrics
    runs-on: ubuntu-24.04
    timeout-minutes: 60
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-validation-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-validation-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install torch torchvision --index-url ${{ env.TORCH_INDEX_URL }}
          pip install -r requirements.txt

      - name: Download models
        run: |
          mkdir -p models
          if [ -f scripts/download_model.py ]; then
            python scripts/download_model.py --model facenet || echo "::warning::Failed to download facenet"
            python scripts/download_model.py --model minifasnetv2 || echo "::warning::Failed to download minifasnetv2"
          fi

      - name: Prepare test dataset
        run: |
          mkdir -p tests/datasets/validation_dataset
          echo "::notice::Using minimal validation dataset"

      - name: Run metrics validation
        id: validation
        run: |
          if [ -f scripts/validate_metrics.py ]; then
            python scripts/validate_metrics.py \
              --dataset tests/datasets/validation_dataset \
              --threshold 0.6 \
              --output validation_results_${{ github.sha }}.json || {
              echo "validation_failed=true" >> $GITHUB_OUTPUT
              echo "::warning::Metrics validation failed"
            }
          else
            echo "::warning::Validation script not found, skipping"
          fi

      - name: Check compliance
        if: steps.validation.outputs.validation_failed != 'true'
        run: |
          if [ -f scripts/check_compliance.py ]; then
            python scripts/check_compliance.py \
              --results validation_results_${{ github.sha }}.json \
              --fail-on-violation || echo "::warning::Compliance check failed"
          fi

      - name: Upload validation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: validation-results
          path: validation_results_*.json
          retention-days: 90

  # --------------------------------------------------------------------------
  # Deploy to Staging
  # --------------------------------------------------------------------------
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-24.04
    timeout-minutes: 15
    needs: [build]
    if: |
      (github.ref == 'refs/heads/develop' || github.event_name == 'workflow_dispatch') &&
      (github.event.inputs.environment == 'staging' || github.event.inputs.environment == '')
    environment:
      name: staging
      url: https://staging-api.company.com
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check Kubernetes secrets
        id: check_k8s
        run: |
          if [ -z "${{ secrets.KUBECONFIG_STAGING }}" ]; then
            echo "configured=false" >> $GITHUB_OUTPUT
            echo "::warning::KUBECONFIG_STAGING not configured, skipping deployment"
          else
            echo "configured=true" >> $GITHUB_OUTPUT
          fi

      - name: Configure kubectl
        if: steps.check_k8s.outputs.configured == 'true'
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Set up kubeconfig
        if: steps.check_k8s.outputs.configured == 'true'
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBECONFIG_STAGING }}" | base64 -d > $HOME/.kube/config
          chmod 600 $HOME/.kube/config

      - name: Deploy to Kubernetes
        if: steps.check_k8s.outputs.configured == 'true'
        run: |
          IMAGE_TAG="$(echo '${{ needs.build.outputs.image_tags }}' | head -n1)"
          kubectl set image deployment/face-recognition-api \
            face-recognition-api="${IMAGE_TAG}" \
            -n staging
          kubectl rollout status deployment/face-recognition-api \
            -n staging \
            --timeout=10m

      - name: Run smoke tests
        if: steps.check_k8s.outputs.configured == 'true'
        run: |
          sleep 30
          curl -f -m 30 https://staging-api.company.com/health || {
            echo "::error::Health check failed"
            exit 1
          }
          curl -f -m 30 https://staging-api.company.com/upload/supported-formats || {
            echo "::error::Upload endpoint check failed"
            exit 1
          }

      - name: Notify Slack
        if: always() && steps.check_k8s.outputs.configured == 'true'
        uses: slackapi/slack-github-action@v1
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        with:
          payload: |
            {
              "text": "${{ job.status == 'success' && 'âœ…' || 'âŒ' }} Staging Deployment: ${{ github.sha }}",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Deployment to Staging ${{ job.status }}*\n\nCommit: `${{ github.sha }}`\nBranch: `${{ github.ref_name }}`\nAuthor: ${{ github.actor }}\nStatus: ${{ job.status }}"
                  }
                }
              ]
            }

  # --------------------------------------------------------------------------
  # Deploy to Production
  # --------------------------------------------------------------------------
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-24.04
    timeout-minutes: 30
    needs: [build, validate-metrics]
    if: |
      github.ref == 'refs/heads/main' ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'production')
    environment:
      name: production
      url: https://api.company.com
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check Kubernetes secrets
        id: check_k8s
        run: |
          if [ -z "${{ secrets.KUBECONFIG_PRODUCTION }}" ]; then
            echo "configured=false" >> $GITHUB_OUTPUT
            echo "::error::KUBECONFIG_PRODUCTION not configured"
            exit 1
          else
            echo "configured=true" >> $GITHUB_OUTPUT
          fi

      - name: Configure kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Set up kubeconfig
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBECONFIG_PRODUCTION }}" | base64 -d > $HOME/.kube/config
          chmod 600 $HOME/.kube/config

      - name: Deploy to Kubernetes (Blue-Green)
        id: deploy
        run: |
          IMAGE_TAG="$(echo '${{ needs.build.outputs.image_tags }}' | head -n1)"
          
          kubectl set image deployment/face-recognition-api-green \
            face-recognition-api="${IMAGE_TAG}" \
            -n production
          
          kubectl rollout status deployment/face-recognition-api-green \
            -n production \
            --timeout=15m

      - name: Run health checks
        run: |
          sleep 60
          
          if [ -f scripts/health_check.sh ]; then
            bash scripts/health_check.sh https://green.api.company.com
          else
            curl -f -m 30 https://green.api.company.com/health || exit 1
          fi

      - name: Switch traffic (Blue â†’ Green)
        id: switch
        run: |
          kubectl patch service face-recognition-api \
            -n production \
            -p '{"spec":{"selector":{"version":"green"}}}'
          
          echo "Traffic switched to green deployment"

      - name: Monitor for 5 minutes
        run: |
          echo "Monitoring green deployment for 5 minutes..."
          sleep 300
          
          if [ -f scripts/monitor_deployment.sh ]; then
            bash scripts/monitor_deployment.sh
          else
            kubectl get pods -n production -l version=green
          fi

      - name: Rollback on failure
        if: failure()
        run: |
          echo "::error::Deployment failed, rolling back to blue"
          kubectl patch service face-recognition-api \
            -n production \
            -p '{"spec":{"selector":{"version":"blue"}}}'
          exit 1

      - name: Tag blue deployment as previous
        if: success()
        run: |
          kubectl label deployment face-recognition-api-blue \
            -n production \
            previous=true \
            --overwrite

      - name: Notify Slack (Success)
        if: success()
        uses: slackapi/slack-github-action@v1
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        with:
          payload: |
            {
              "text": "ðŸš€ Production Deployment Successful: ${{ github.sha }}",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Production Deployment Complete*\n\nCommit: `${{ github.sha }}`\nAuthor: ${{ github.actor }}\n\nâœ… All health checks passed\nðŸ”„ Traffic switched to new deployment"
                  }
                }
              ]
            }

      - name: Notify Slack (Failure)
        if: failure()
        uses: slackapi/slack-github-action@v1
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        with:
          payload: |
            {
              "text": "âŒ Production Deployment Failed: ${{ github.sha }}",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Production Deployment Failed*\n\nCommit: `${{ github.sha }}`\nAuthor: ${{ github.actor }}\n\nâš ï¸ Rollback initiated\nðŸ” Check: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                }
              ]
            }

  # --------------------------------------------------------------------------
  # Post-deployment verification
  # --------------------------------------------------------------------------
  post-deploy-verification:
    name: Post-Deployment Verification
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    needs: [deploy-production]
    if: success() && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run E2E tests against production
        run: |
          curl -f -m 30 https://api.company.com/health || exit 1
          curl -f -m 30 https://api.company.com/metrics || exit 1
          
          echo "âœ… All post-deployment checks passed"

      - name: Update deployment status
        run: |
          echo "Deployment verified at $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
          echo "Commit: ${{ github.sha }}"
          echo "Version: ${{ github.ref_name }}"

  # --------------------------------------------------------------------------
  # Summary Job (always runs)
  # --------------------------------------------------------------------------
  summary:
    name: Pipeline Summary
    runs-on: ubuntu-24.04
    if: always()
    needs: [lint, security, unit-tests, integration-tests, build, deploy-staging, deploy-production]
    steps:
      - name: Generate summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          # ðŸš€ CI/CD Pipeline Summary
          
          ## Jobs Status
          
          | Job | Status |
          |-----|--------|
          | ðŸ” Lint | ${{ needs.lint.result == 'success' && 'âœ… Passed' || needs.lint.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          | ðŸ”’ Security | ${{ needs.security.result == 'success' && 'âœ… Passed' || needs.security.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          | ðŸ§ª Unit Tests | ${{ needs.unit-tests.result == 'success' && 'âœ… Passed' || needs.unit-tests.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          | ðŸ”— Integration Tests | ${{ needs.integration-tests.result == 'success' && 'âœ… Passed' || needs.integration-tests.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          | ðŸ³ Build | ${{ needs.build.result == 'success' && 'âœ… Passed' || needs.build.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          | ðŸŽ¯ Deploy Staging | ${{ needs.deploy-staging.result == 'success' && 'âœ… Passed' || needs.deploy-staging.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          | ðŸš€ Deploy Production | ${{ needs.deploy-production.result == 'success' && 'âœ… Passed' || needs.deploy-production.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |
          
          ## Build Information
          
          - **Commit:** `${{ github.sha }}`
          - **Branch:** `${{ github.ref_name }}`
          - **Author:** ${{ github.actor }}
          - **Triggered by:** ${{ github.event_name }}
          - **Workflow run:** [#${{ github.run_number }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          
          ## Artifacts
          
          EOF
          
          if [ "${{ needs.build.result }}" == "success" ]; then
            IMAGE_TAG="$(echo '${{ needs.build.outputs.image_tags }}' | head -n1)"
            echo "- ðŸ³ Docker Image: \`${IMAGE_TAG}\`" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Pipeline completed at:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY

      - name: Check overall status
        run: |
          FAILED_JOBS=""
          
          if [ "${{ needs.lint.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}lint "
          fi
          if [ "${{ needs.security.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}security "
          fi
          if [ "${{ needs.unit-tests.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}unit-tests "
          fi
          if [ "${{ needs.integration-tests.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}integration-tests "
          fi
          if [ "${{ needs.build.result }}" == "failure" ]; then
            FAILED_JOBS="${FAILED_JOBS}build "
          fi
          
          if [ -n "$FAILED_JOBS" ]; then
            echo "::error::Pipeline failed. Failed jobs: ${FAILED_JOBS}"
            exit 1
          fi
          
          echo "âœ… Pipeline completed successfully"
