#!/bin/bash
# =============================================================================
# Entrypoint Script –¥–ª—è Face Recognition Service
# =============================================================================
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ
#
# Environment variables:
#   LOCAL_ML_ENABLE_CUDA=true/false (auto-detect by default)
#   LOCAL_ML_DEVICE=cuda/cpu (auto-detect by default)
# =============================================================================

set -e

echo "========================================"
echo "üöÄ Face Recognition Service Startup"
echo "========================================"
echo ""

# 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
echo "1Ô∏è‚É£  Environment Detection"
ENVIRONMENT=${ENVIRONMENT:-production}
echo "   ENVIRONMENT: $ENVIRONMENT"

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
echo ""
echo "2Ô∏è‚É£  GPU Detection"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ nvidia-smi
if command -v nvidia-smi &> /dev/null; then
    GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits 2>/dev/null | head -1 || echo "Unknown")
    echo "   ‚úÖ NVIDIA GPU detected: $GPU_INFO"
    GPU_AVAILABLE=true
else
    echo "   ‚ö†Ô∏è  NVIDIA GPU not detected (nvidia-smi not found)"
    GPU_AVAILABLE=false
fi

# –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA —á–µ—Ä–µ–∑ PyTorch
PYTHON_GPU_CHECK=$(python3 -c "
import torch
if torch.cuda.is_available():
    print(f'cuda:{torch.cuda.current_device()}', end='')
else:
    print('cpu', end='')
" 2>/dev/null || echo "unknown")

echo "   üì± PyTorch device: $PYTHON_GPU_CHECK"

# 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è ML
echo ""
echo "3Ô∏è‚É£  ML Environment Configuration"

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ CUDA
if [ "$LOCAL_ML_ENABLE_CUDA" != "false" ] && [ "$GPU_AVAILABLE" = "true" ]; then
    export LOCAL_ML_ENABLE_CUDA=true
    export LOCAL_ML_DEVICE=cuda
    export TORCH_CUDA_ARCH_LIST="6.0;7.0;7.5;8.0;8.6;8.9;9.0"  # GPU architectures
    echo "   ‚úÖ CUDA enabled"
else
    export LOCAL_ML_ENABLE_CUDA=false
    export LOCAL_ML_DEVICE=cpu
    echo "   ‚ÑπÔ∏è  Using CPU"
fi

echo "   LOCAL_ML_ENABLE_CUDA: $LOCAL_ML_ENABLE_CUDA"
echo "   LOCAL_ML_DEVICE: $LOCAL_ML_DEVICE"

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π
echo ""
echo "4Ô∏è‚É£  Model Loading Check"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–µ–π
if [ -d "/app/models" ]; then
    MODEL_COUNT=$(find /app/models -name "*.pth" -o -name "*.pt" 2>/dev/null | wc -l)
    echo "   üì¶ Models found: $MODEL_COUNT"
else
    echo "   ‚ÑπÔ∏è  Models directory not found (will download on first run)"
fi

# 5. –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
if [ "$GPU_HEALTH_CHECK" = "true" ]; then
    echo ""
    echo "5Ô∏è‚É£  Running GPU Health Check..."
    python3 /app/scripts/check_gpu.py || true
fi

# 6. –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–ø—É—Å–∫–µ
echo ""
echo "========================================"
echo "üåê Starting Service"
echo "========================================"
echo ""
echo "   API Docs: http://localhost:8000/docs"
echo "   Health:   http://localhost:8000/health"
echo ""

# 7. –ó–∞–ø—É—Å–∫ uvicorn
exec uvicorn app.main:app "$@"